# =====================================================
# Chatbot RAG Local com LangChain + llama.cpp (limitando contexto)
# =====================================================

from langchain_community.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings  # pacote atualizado
from langchain_community.vectorstores import Chroma
from langchain.schema.runnable import RunnableMap
from langchain.schema.output_parser import StrOutputParser
import sys
import textwrap  # ‚úÖ para formatar a sa√≠da no terminal

# 1. Carregar documento PDF
loader = PyPDFLoader("relatorio.pdf")
documents = loader.load()

# 2. Criar embeddings e base vetorial Chroma
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ‚ö†Ô∏è Apague a pasta ./chroma_db se trocar o modelo de embeddings
vectorstore = Chroma.from_documents(documents, embeddings, persist_directory="./chroma_db")

# 3. Configurar o mecanismo de recupera√ß√£o (reduzido para 1 documento)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# 4. Template do prompt revisado
template = """[INST]
Voc√™ √© um assistente t√©cnico especializado em relat√≥rios de est√°gio.
Com base exclusivamente no contexto abaixo, descreva detalhadamente as atividades desenvolvidas,
utilizando frases completas e linguagem t√©cnica formal.

Se a pergunta for sobre as atividades do est√°gio, utilize as se√ß√µes 3.1 a 3.7 do contexto.
Se n√£o houver informa√ß√£o suficiente, diga apenas "N√£o sei com base no documento.".

=== CONTEXTO ===
{context}
=== FIM DO CONTEXTO ===

Pergunta: {question}
Responda de forma estruturada e completa, listando cada atividade de forma clara.
[/INST]"""



prompt = PromptTemplate.from_template(template)

# 5. Carregar o modelo LlamaCpp
llm = LlamaCpp(
    model_path = "/home/demarco/√Årea de trabalho/TCC/models/llama-2-7b-chat.Q4_K_M.gguf",
    n_ctx=4096,         # tamanho m√°ximo do contexto
    n_threads=8,        # n√∫mero de threads CPU
    n_gpu_layers=40,    # mais camadas na GPU (aproveita melhor a RTX 3060)
    max_tokens=1024,    # permite respostas mais longas
    streaming=False,     # mostra tokens conforme gerados
    temperature=0.7,    # deixa as respostas mais elaboradas
    top_p=0.9,
    verbose=True
)


# 6. Montar a cadeia RAG (limitando o tamanho do contexto)
rag_chain = (
    RunnableMap({
        "context": lambda x: "\n".join(
    [doc.page_content[:1500] for doc in retriever.invoke(x["question"])],  # ‚úÖ limite de 2000 caracteres
        ),
        "question": lambda x: x["question"]
    })
    | prompt
    | llm
    | StrOutputParser()
)

# 7. Loop de chat interativo
print("Chatbot RAG Local (LangChain + llama.cpp)")
print("Digite 'sair' para encerrar a conversa.\n")

while True:
    try:
        pergunta = input("Voc√™: ")
        if pergunta.lower() in {"sair", "exit", "quit"}:
            print("IA: At√© logo!")
            break

        resposta = rag_chain.invoke({"question": pergunta})
        resposta_limpa = " ".join(resposta.split())

        # ‚úÖ Formatar sa√≠da para melhor leitura
        resposta_formatada = textwrap.fill(resposta_limpa, width=120)

        print("\nüß† Resposta:\n")
        print(resposta_formatada)
        print("\n" + "=" * 80 + "\n")
        sys.stdout.flush()

    except KeyboardInterrupt:
        print("\nIA: Encerrando execu√ß√£o.")
        break
